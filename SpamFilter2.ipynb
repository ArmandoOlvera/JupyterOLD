{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "##from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "def progress(i, end_val, bar_length=50):\n",
    "    '''\n",
    "    Print a progress bar of the form: Percent: [#####      ]\n",
    "    i is the current progress value expected in a range [0..end_val]\n",
    "    bar_length is the width of the progress bar on the screen.\n",
    "    '''\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "NEWLINE = '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "\n",
    "SOURCES = [\n",
    "    ('spam/spam',        SPAM),\n",
    "    ('spam/easy_ham',    HAM),\n",
    "    ('spam/hard_ham',    HAM),\n",
    "    ('spam/beck-s',      HAM),\n",
    "    ('spam/farmer-d',    HAM),\n",
    "    ('spam/kaminski-v',  HAM),\n",
    "    ('spam/kitchen-l',   HAM),\n",
    "    ('spam/lokay-m',     HAM),\n",
    "    ('spam/williams-w3', HAM),\n",
    "    ('spam/BG',          SPAM),\n",
    "    ('spam/GP',          SPAM),\n",
    "    ('spam/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    '''\n",
    "    Generator of pairs (filename, filecontent)\n",
    "    for all files below path whose name is not in SKIP_FILES.\n",
    "    The content of the file is of the form:\n",
    "        header....\n",
    "        <emptyline>\n",
    "        body...\n",
    "    This skips the headers and returns body only.\n",
    "    '''\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "\n",
    "def build_data_frame(l, path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for i, (file_name, text) in enumerate(read_files(path)):\n",
    "        if ((i+l) % 100 == 0):\n",
    "            progress(i+l, 58910, 50)##Para ir mostrando el porcentaje de carga de datos\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "   \n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame, len(rows)\n",
    "\n",
    "def load_data():\n",
    "    data = DataFrame({'text': [], 'class': []})\n",
    "    l = 0\n",
    "    for path, classification in SOURCES:\n",
    "        data_frame, nrows = build_data_frame(l, path, classification)\n",
    "        data = data.append(data_frame,sort=False)\n",
    "        l += nrows\n",
    "    data = data.reindex(numpy.random.permutation(data.index))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "# Cargamos los datos de la carpeta spam, donde se tienen los dataset de los correos\n",
    "data=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58910"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) ##mostramos el largo de caracteristicas obtenidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam/kitchen-l/_americas_culture/19</th>\n",
       "      <td>The Management Team follow-up meeting for the ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam/GP/part11/msg3330.eml</th>\n",
       "      <td>&lt;html&gt;\\n\\n&lt;head&gt;\\n\\n&lt;title&gt;You got it!&lt;/title&gt;...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam/lokay-m/corporate/613</th>\n",
       "      <td>Payroll deductions and cost center billings fo...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam/GP/part3/msg5709.eml</th>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam/BG/2005/02/1109546715.6669_486.txt</th>\n",
       "      <td>------775aa55dfc-glay.org\\n\\nContent-Type: tex...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      text  \\\n",
       "spam/kitchen-l/_americas_culture/19      The Management Team follow-up meeting for the ...   \n",
       "spam/GP/part11/msg3330.eml               <html>\\n\\n<head>\\n\\n<title>You got it!</title>...   \n",
       "spam/lokay-m/corporate/613               Payroll deductions and cost center billings fo...   \n",
       "spam/GP/part3/msg5709.eml                <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...   \n",
       "spam/BG/2005/02/1109546715.6669_486.txt  ------775aa55dfc-glay.org\\n\\nContent-Type: tex...   \n",
       "\n",
       "                                        class  \n",
       "spam/kitchen-l/_americas_culture/19       ham  \n",
       "spam/GP/part11/msg3330.eml               spam  \n",
       "spam/lokay-m/corporate/613                ham  \n",
       "spam/GP/part3/msg5709.eml                spam  \n",
       "spam/BG/2005/02/1109546715.6669_486.txt  spam  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()#se muestran algunos datos ya con su texto y su etiqueta y su index como su direccion de archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Antes de entrenar los algoritmos, necesitamos extraer las caracteristicas de los emails, con el fin de reducir la cantidad \n",
    "#De datos que se procesaran en un set de atributos del que pueda aprender mas, para eso se usa COuntVectorizer\n",
    "#de sklearn y magicamente hace operaciones para clasificar y entrenar con feature_extraction\n",
    "# por otra parte  fit_transform realiza las tareas de aprender del vocabulario y extraer las caraceteristicas del \n",
    "#vocabulario\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "counts = count_vectorizer.fit_transform(data['text'].values)\n",
    "targets = data['class'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'idf_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5030675ba068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'idf_'"
     ]
    }
   ],
   "source": [
    "#Se usara para este caso el clasificador de bayes y la definicion es la siguiente:\n",
    "#En términos simples, un clasificador de Bayes ingenuo asume que la presencia o ausencia de una característica \n",
    "#particular no está relacionada con la presencia o ausencia de cualquier otra característica, dada la clase variable.\n",
    "#Por ejemplo, una fruta puede ser considerada como una manzana si es roja, redonda y de alrededor de 7 cm de diámetro\n",
    "# Un clasificador de Bayes ingenuo considera que cada una de estas características contribuye de manera \n",
    "#independiente a la probabilidad de que esta fruta sea una manzana, independientemente de la presencia o \n",
    "#ausencia de las otras características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "targets = data['class'].values\n",
    "classifier.fit(counts, targets)#se entrena el modelo para hacer predicciones con bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Ejemplo propuesto por la fuente\n",
    "examples = ['Free Viagra call today!', \"I'm going to attend the Linux users group tomorrow.\"]\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions # [1, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EJemplo propuesto por nosotros Alfredo y Armando jsjsj, parece que la palabra #viagra# y #free# sale en la mayoria\n",
    "#de los ejemplos de spam, se pueden proponer otros ejemplos para probar como el clasificador de peliculas que se vio\n",
    "# en clase, por lo que si se omiten este tipo de palabras probablemente dira que el mensaje no es spam.\n",
    "examples = ['Join to us, and get our free trial right now', \"Hello, nice to meet you.\"]\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions # [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "#creamos un pipeline para poder ejecutar los clasificadores antes mencionados\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(data['text'].values, data['class'].values)\n",
    "pipeline.predict(examples) # ['spam', 'ham'] esperados con el ultimo ejemplo propuesto...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 58910\n",
      "Score: 0.9364936082702466\n",
      "Confusion matrix:\n",
      "[[23400   139]\n",
      " [ 4102 31269]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "k_fold = KFold( n_splits=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold.split(counts):\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "\n",
    "    #obtenemos los resultados de usar la validacion cruzada\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
